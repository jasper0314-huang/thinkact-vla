<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning">
  <meta property="og:title" content="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning"/>
  <meta property="og:description" content="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning"/>
  <meta property="og:url" content="https://jasper0314-huang.github.io/thinkact-vla/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning">
  <meta name="twitter:description" content="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser.png">
  <meta name="twitter:card" content="static/images/teaser.png">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="NeurIPS 2025, Vision-Language-Action Models, Large Reasoning Models, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="icon" type="image/png" href="static/favicon/favicon-96x96.png" sizes="96x96" />
  <link rel="icon" type="image/svg+xml" href="static/favicon/favicon.svg" />
  <link rel="shortcut icon" href="static/favicon/favicon.ico" />
  <link rel="apple-touch-icon" sizes="180x180" href="static/favicon/apple-touch-icon.png" />
  <meta name="apple-mobile-web-app-title" content="ThinkAct" />
  <link rel="manifest" href="static/favicon/site.webmanifest" />

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1FXRQXW9N3"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1FXRQXW9N3');
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="static/images/thinkact_logo.png" alt="Logo" style="height: 65px; vertical-align: middle; margin-right: -10px; position: relative; bottom: 18px;">
              <span>ThinkAct:</span>
              <br>
              <span style="font-size: 2.5rem;">Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</span>
            </h1>

            <div class="is-size-5 publication-authors" style="margin-bottom: 5px; margin-top: -10px;">
              <span class="author-block">
                <sup style="color: #76B900; font-size: 1.8rem; font-weight: bold;">NeurIPS 2025</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block" style="margin-right: 15px;">
                <a href="https://jasper0314-huang.github.io/" target="_blank">Chi-Pin Huang</a>
                <sup>1,2†</sup>
              </span>

              <span class="author-block" style="margin-right: 15px;">
                <a href="https://kristery.github.io/" target="_blank">Yueh-Hua Wu</a>
                <sup>1</sup>
              </span>

              <span class="author-block" style="margin-right: 15px;">
                <a href="https://minhungchen.netlify.app/" target="_blank">Min-Hung Chen</a>
                <sup>1</sup>
              </span>

              <!-- <br> -->
              <span class="author-block" style="margin-right: 15px;">
                <a href="https://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>
                <sup>1,2</sup>
              </span>

              <span class="author-block" style="margin-right: 15px;">
                <a href="https://fuenyang1127.github.io/" target="_blank">Fu-En Yang</a>
                <sup>1</sup>
              </span>

            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>
                NVIDIA&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>2</sup>
                National Taiwan University
                <br>
                <span class="eql-cntrb">
                  <sup>†</sup>Work done during internship
                  <br>
                </span>
                <!-- <br> -->
                <div class="columns is-centered" style="margin-top: 0.0rem; display: flex; align-items: center;">
                  <div class="column is-narrow">
                    <img src="static/images/nvidia_logo.png" alt="NVIDIA" style="height: 45px;">
                  </div>
                  <div style="width: 1.3rem;"></div>
                  <div class="column is-narrow">
                    <img src="static/images/ntu_logo1.png" alt="NTU" style="height: 50px;">
                  </div>
                </div>
                <!-- <br> -->
                <!-- <sup style="color: rgba(171, 20, 20, 0.874); font-size: 1.8rem; font-weight: bold;">CVPR 2025</sup> -->
              </span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2507.16815" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://jasper0314-huang.github.io/thinkact-vla/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>

                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2507.16815" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="ThinkAct-Teaser" class="center-image" style="max-width: 100%; height: auto; margin-bottom: 20px;" />
      <h2 class="subtitle has-text-justified">
        <p>
          We introduce <b>ThinkAct</b>, a reasoning VLA framework capable of thinking before acting. Through reasoning reinforced by our <b>action-aligned visual feedback</b>, ThinkAct enables capabilities of few-shot adaptation, long-horizon planning, and self-correction in embodied tasks.
        </p>
      </h2>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3 has-text-centered">Method</h2>
      <img src="static/images/method.png" alt="ThinkAct-Method" class="center-image" style="max-width: 100%; height: auto;" />
      <p class="has-text-centered has-text-justified" style="font-size: 1.0rem; line-height: 1.3;">
        Overview of ThinkAct. (a) Given observation \( o_t \) and instruction \( l \), ThinkAct advances action-aligned rewards derived from visual trajectory \( \tau \) to incentivize embodied reasoning capability of reasoning MLLM \( F_\theta \). (b) Conditioned on the visual plan latent \( c_t \), the DiT-based Action Model \( \pi_\phi \) learns to predict executable action while keeping \( F_\theta \) frozen. Note that, during inference, \( \pi_\phi \) and \( F_\theta \) could operate asynchronously to enable slow thinking and fast control for VLA reasoning tasks.
      </p>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body has-text-centered">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Experiment Results</h2>

      <h3 class="title is-4">Robot Manipulation Tasks</h3>
      <div style="max-width: 60%; margin: 0 auto; text-align: justify; margin-top: -10px;">
        <p class="has-text-centered has-text-justified" style="margin-bottom: 3px;">
          Quantitative comparisons of robot manipulation tasks on SimplerEnv and LIBERO benchmarks. Bold denotes the best result.
        </p>
        <img src="static/images/robot_table.png" alt="ThinkAct-Robot-Table" class="center-image" style="width: 100%; height: auto; margin-top: 5px;" />
      </div>

      <h3 class="title is-4" style="margin-top: 20px;">Embodied Reasoning Tasks</h3>
      <div style="max-width: 60%; margin: 0 auto; text-align: justify; margin-top: -10px;">
        <p class="has-text-centered has-text-justified" style="margin-bottom: 3px;">
          Quantitative comparisons of embodied reasoning tasks on EgoPlan-Bench2, RoboVQA, and OpenEQA benchmarks. Note that, Qwen2.5-VL* indicates fine-tuning the original Qwen2.5-VL using EgoPlan-IT and RoboVQA datasets. Bold denotes the best result.
        </p>
        <img src="static/images/qa_table.png" alt="ThinkAct-QA-Table" class="center-image" style="max-width: 100%; height: auto; margin-top: 5px;" />
      </div>

      <h3 class="title is-4" style="margin-top: 20px;">Few-shot Adaptation</h3>
      <div style="max-width: 60%; margin: 0 auto; text-align: justify; margin-top: -10px;">
        <p class="has-text-centered has-text-justified">
          Few-shot adaptation results on LIBERO. We use 5 and 10 demonstrations per task for fine-tuning.
        </p>
        <img src="static/images/few_shot.png" alt="ThinkAct-Few-shot" class="center-image" style="max-width: 100%; height: auto; margin-top: 5px;" />
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body has-text-centered">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Visualization</h2>

        <h3 class="title is-4">Diverse Scene and Long-horizon Robot Manipulation Tasks</h3>
        <div style="max-width: 60%; margin: 0 auto; text-align: justify; margin-top: -10px;">
          <p class="has-text-centered has-text-justified">
            Qualitative results of intermediate reasoning steps and visualized trajectory for robot manipulation tasks on SimplerEnv and LIBERO benchmarks.
          </p>
        </div>
        <div id="manipulation-carousel" class="carousel results-carousel" style="margin-bottom: 20px;">
          <div class="item item-manipulation_1">
            <video poster="" id="manipulation_1" autoplay controls muted loop playsinline width="75%" style="margin-bottom: 20px;">
              <!-- Your video file here -->
              <source src="static/videos/manipulation_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-manipulation_2">
            <video poster="" id="manipulation_2" autoplay controls muted loop playsinline width="75%" style="margin-bottom: 20px;">
              <!-- Your video file here -->
              <source src="static/videos/manipulation_2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-manipulation_3">
            <video poster="" id="manipulation_3" autoplay controls muted loop playsinline width="75%" style="margin-bottom: 20px;">
              <!-- Your video file here -->
              <source src="static/videos/manipulation_3.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <h3 class="title is-4" style="margin-top: 40px;">Reflection & Self-correction</h3>
        <div style="max-width: 60%; margin: 0 auto; text-align: justify; margin-top: -10px;">
          <p class="has-text-centered has-text-justified">
            Demonstration of self-reflection and correction capability of ThinkAct. The reasoning MLLM identifies the failure and generates a revised plan that guides the action model to complete the task.
          </p>
        </div>
        <div id="reflection-carousel" class="carousel results-carousel" style="margin-bottom: 20px; margin-top: -15px;">
          <div class="item item-reflection_1">
            <video poster="" id="reflection_1" autoplay controls muted loop playsinline width="75%" style="margin-bottom: 20px;">
              <!-- Your video file here -->
              <source src="static/videos/reflection_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-reflection_2">
            <video poster="" id="reflection_2" autoplay controls muted loop playsinline width="75%" style="margin-bottom: 20px;">
              <!-- Your video file here -->
              <source src="static/videos/reflection_2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-reflection_3">
            <video poster="" id="reflection_3" autoplay controls muted loop playsinline width="75%" style="margin-bottom: 20px;">
              <!-- Your video file here -->
              <source src="static/videos/reflection_3.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <h3 class="title is-4" style="margin-top: 40px;">Qualitative Comparisons of Manipulation Results</h3>
        <div style="max-width: 60%; margin: 0 auto; text-align: justify; margin-top: -10px;">
          <p class="has-text-centered has-text-justified">
            Qualitative comparisons of robot execution results between DiT-Policy, OpenVLA, and ThinkAct.
          </p>
        </div>
        <div class="columns is-centered" style="margin-top: 15px;">
          <div class="column is-one-third has-text-centered">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/comparison_1.mp4" type="video/mp4">
            </video>
            <text>“Put carrot on plate”</text>
          </div>
          <div style="width: 2.5rem;"></div>
          <div class="column is-one-third has-text-centered">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/comparison_2.mp4" type="video/mp4">
            </video>
            <text>“Close bottom drawer”</text>
          </div>
        </div>
      
        <div class="columns is-centered" style="margin-top: -15px;">
          <div class="column is-one-third has-text-centered">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/comparison_3.mp4" type="video/mp4">
            </video>
            <text>“Pick up the black bowl next to the cookie box and place it on the plate”</text>
          </div>
          <div style="width: 2.5rem;"></div>
          <div class="column is-one-third has-text-centered">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/comparison_4.mp4" type="video/mp4">
            </video>
            <text>“Pick up the ketchup and place it in the basket”</text>
          </div>
        </div>

        <div class="columns is-centered" style="margin-top: -15px;">
          <div class="column is-one-third has-text-centered">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/comparison_5.mp4" type="video/mp4">
            </video>
            <text>“Open the top drawer and put the bowl inside”</text>
          </div>
          <div style="width: 2.5rem;"></div>
          <div class="column is-one-third has-text-centered">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/comparison_6.mp4" type="video/mp4">
            </video>
            <text>“Turn on the stove and put the moka pot on it”</text>
          </div>
        </div>

        <h3 class="title is-4" style="margin-top: 40px;">Embodied Reasoning</h3>
        <div style="max-width: 60%; margin: 0 auto; text-align: justify; margin-top: -10px;">
          <p class="has-text-centered has-text-justified">
            Visualization of reasoning process and the derived answer for embodied reasoning tasks on EgoPlan-Bench2, OpenEQA, and RoboVQA benchmarks.
          </p>
        </div>
        <div class="container has-text-centered" style="margin-top: 15px;">
          <div id="results-carousel" class="carousel results-carousel" style="margin-bottom: 20px;">
            
            <!-- Slide 1 -->
            <div class="columns is-vcentered is-centered" style="margin-bottom: 20px;">
              <!-- Left column: Video + QA -->
              <div class="column is-4">
                <video autoplay controls muted loop playsinline width="80%">
                  <source src="static/videos/qa_1.mp4" type="video/mp4">
                </video>
                <div class="content mt-4 has-text-justified">
                  <p><strong>Question:</strong> Considering the progress shown in the video and the observation in the final frame, what should be the next action to complete the task: "prepare baking powder"<br>
                    A. close box<br>
                    B. remove baking powder sachet<br>
                    C. pour baking powder into basin<br>
                    D. put salt on table
                  </p>
                  <p><strong>Answer:</strong> C. pour baking powder into basin</p>
                </div>
              </div>
        
              <!-- Right column: Reasoning -->
              <div class="column is-5 has-text-justified">
                <div class="box content">
                  <p>&lt;think&gt;<br>
                  Let me think about this carefully. The person is preparing something in a green bowl. <span style="color: #76b900;">They have already added salt, and now they are dealing with a yellow packet labeled "Mandazi."</span> The action seems to be related to preparing a mixture, possibly for baking.</p>
        
                  <p>Hmm, the person is opening the yellow packet and seems to be taking something out. Wait, is it a baking powder? It looks like they are pouring something from the packet into the bowl.</p>
        
                  <p>Oh, I see. The person is likely preparing a mixture that requires baking powder. Let's break it down: <span style="color: #76b900;">they have salt and now they are adding something from the packet. It must be the baking powder.</span></p>
        
                  <p>Therefore, the next action should be to <span style="color: #76b900;">pour the baking powder into the basin</span>. This aligns with the process of preparing baking powder as part of the recipe.<br>
                  &lt;/think&gt;<br>
                  &lt;answer&gt;<strong><span style="color: #76b900;">C</span></strong>&lt;/answer&gt;</p>
                </div>
              </div>
            </div>
        
            <!-- Slide 2 -->
            <div class="columns is-vcentered is-centered">
              <!-- Left column -->
              <div class="column is-4">
                <video autoplay controls muted loop playsinline width="80%">
                  <source src="static/videos/qa_2.mp4" type="video/mp4">
                </video>
                <div class="content mt-4 has-text-justified">
                  <p><strong>Question:</strong> What shape is the carpet on the floor?
                  </p>
                  <p><strong>Answer:</strong> Rectangular</p>
                </div>
              </div>
        
              <!-- Right column -->
              <div class="column is-5 has-text-justified">
                <div class="box content">
                  <p>&lt;think&gt;<br>
                  Let me think about this for a moment. The image shows a room with a carpet on the floor. I'm trying to figure out its shape.</p>

                  <p>At first glance, the floral pattern in the center draws attention, which might make the shape seem more complex. But looking closely at the edges, <span style="color: #76b900;">they are straight and meet at right angles</span>. The corners are clearly defined, and <span style="color: #76b900;">the sides are parallel.</span></p>

                  <p>So, despite the decorative design, <span style="color: #76b900;">the overall outline of the carpet is rectangular</span>. That seems clear now.<br>
                  &lt;/think&gt;<br>
                  &lt;answer&gt;<strong><span style="color: #76b900;">The carpet on the floor is rectangular.</span></strong>&lt;/answer&gt;</p>
                </div>
              </div>
            </div>
            
            <!-- Slide 3 -->
            <div class="columns is-vcentered is-centered">
              <!-- Left column -->
              <div class="column is-4">
                <video autoplay controls muted loop playsinline width="80%">
                  <source src="static/videos/qa_3.mp4" type="video/mp4">
                </video>
                <div class="content mt-4 has-text-justified">
                  <p><strong>Question:</strong> Move the paper to left. Is it possible right now?</p>
                  <p><strong>Answer:</strong> yes</p>
                </div>
              </div>
        
              <!-- Right column -->
              <div class="column is-5 has-text-justified">
                <div class="box content">
                  <p>
                    &lt;think&gt;<br>
                    Let me think about this for a moment. The image shows a desk with various items on it, and a robotic arm is currently holding a piece of paper above the desk.
                  </p>

                  <p>
                    The question is whether the paper can be moved to the left right now. <span style="color: #76b900;">Since the paper is already lifted off the surface, the arm has full control of its movement.</span>
                  </p>

                  <p>
                    Given that, <span style="color: #76b900;">there's nothing preventing it from being moved left</span>. So yes, it's possible.<br>
                    &lt;/think&gt;<br>
                    &lt;answer&gt;<strong><span style="color: #76b900;">yes</span></strong>&lt;/answer&gt;
                  </p>
                </div>
              </div>
            </div>
        
          </div>
        </div>
      </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{huang2025thinkact,
  title={Thinkact: Vision-language-action reasoning via reinforced visual latent planning},
  author={Huang, Chi-Pin and Wu, Yueh-Hua and Chen, Min-Hung and Wang, Yu-Chiang Frank and Yang, Fu-En},
  journal={arXiv preprint arXiv:2507.16815},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
